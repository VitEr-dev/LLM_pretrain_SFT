# LLM: pre-train and post-train(SFT_LoRA)

Two projects:
-->  pretrain: Llama on Russian Novels dataset
-->  post-train: SFT+LoRA Qwen on d0rj/alpaca-cleaned-ru (instruct) dataset
