{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets trl tqdm accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: \n",
    "# –ò–º–ø–æ—Ä—Ç—ã –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "print(\"–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ:\")\n",
    "print(f\"-->–î–æ—Å—Ç—É–ø–µ–Ω CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"-->–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: \n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "def load_and_format_dataset():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\"\"\"\n",
    "    \n",
    "    print(\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...\")\n",
    "    ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "    \n",
    "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "    def format_alpaca_to_chat(example):\n",
    "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä Alpaca –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Qwen\"\"\"\n",
    "        if example['input']:\n",
    "            user_message = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
    "        else:\n",
    "            user_message = example['instruction']\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        \n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ Qwen\n",
    "        formatted_text = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                formatted_text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n",
    "            else:\n",
    "                formatted_text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n",
    "        \n",
    "        return {\"text\": formatted_text}\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "    print(\"üîÑ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
    "    formatted_ds = ds.map(format_alpaca_to_chat)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ\n",
    "    print(\"\\n–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "    print(formatted_ds[11]['text'][:500] + \"...\")\n",
    "    print(f\"\\n‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(formatted_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    \n",
    "    return formatted_ds\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "formatted_ds = load_and_format_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: \n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –î–û –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def test_before_training():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    print(\"–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –î–û –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    questions_rus = [\n",
    "        \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "        \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\",\n",
    "        \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "        \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(questions_rus, 1):\n",
    "        print(f\"\\n{i}. –í–æ–ø—Ä–æ—Å: {question}\")\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"–û—Ç–≤–µ—Ç: {answer}\")\n",
    "        print(\"‚Äï\" * 55)\n",
    "\n",
    "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "test_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: \n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SFT —Å LoRA\n",
    "\n",
    "def setup_training():\n",
    "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å SFTConfig\"\"\"\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "    print(f\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=8, \n",
    "        lora_alpha=16, \n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º SFTConfig \n",
    "    cfg = SFTConfig(\n",
    "        output_dir=\"./qwen2.5-0.5b-sft-ru\",\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=1024,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  \n",
    "        logging_steps=50,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        report_to=None,\n",
    "        run_name='qwen-lora-sft',\n",
    "        save_steps=500,\n",
    "        fp16=True,  # –í–∫–ª—é—á–∞–µ–º mixed precision –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "        optim=\"adamw_torch\",\n",
    "        remove_unused_columns=False,\n",
    "        packing=True,  # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, peft_cfg, cfg\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "model, tokenizer, peft_cfg, cfg = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: –ó–∞–ø—É—Å–∫ SFT –æ–±—É—á–µ–Ω–∏—è —Å LoRA\n",
    "def create_and_train_model(formatted_ds):\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å SFTConfig\"\"\"\n",
    "    \n",
    "    print(\"üîÑ –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä...\")\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º tokenizer –≤–º–µ—Å—Ç–æ processing_class\n",
    "        args=cfg,\n",
    "        train_dataset=formatted_ds,\n",
    "        peft_config=peft_cfg,\n",
    "        dataset_text_field=\"text\",  # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø–æ–ª—è\n",
    "        max_seq_length=1024,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º max_seq_length\n",
    "        packing=True,  # –í–∫–ª—é—á–∞–µ–º packing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    )\n",
    "    \n",
    "    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "    print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(\"./qwen2.5-0.5b-sft-ru\")\n",
    "    \n",
    "    print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "    return trainer\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "trainer = create_and_train_model(formatted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è\n",
    "def test_after_training():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    print(\"\\n–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model_path = \"./qwen2.5-0.5b-sft-ru\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    questions_rus = [\n",
    "        \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "        \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\",\n",
    "        \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "        \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"–û—Ç–≤–µ—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\\n\")\n",
    "    \n",
    "    for i, question in enumerate(tqdm(questions_rus, desc=\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\"), 1):\n",
    "        print(f\"\\n{i}. –í–æ–ø—Ä–æ—Å: {question}\")\n",
    "        \n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ\n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "            )\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç\n",
    "        response = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"–û—Ç–≤–µ—Ç: {answer.strip()}\")\n",
    "        print(\"‚Äï\" * 55)\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "test_after_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏\n",
    "def cleanup_memory():\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU\"\"\"\n",
    "    import gc\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n",
    "\n",
    "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
