{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM: post-train SFT + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/.venv/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.venv/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: trl in /home/ubuntu/.venv/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.venv/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /home/ubuntu/.venv/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.venv/lib/python3.10/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch datasets trl tqdm accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\n",
      "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ:\n",
      "-->–î–æ—Å—Ç—É–ø–µ–Ω CUDA: True\n",
      "GPU: Tesla T4\n",
      "–ü–∞–º—è—Ç—å GPU: 14.6 GB\n"
     ]
    }
   ],
   "source": [
    "# 1: \n",
    "# –ò–º–ø–æ—Ä—Ç—ã –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "print(\"–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ:\")\n",
    "print(f\"-->–î–æ—Å—Ç—É–ø–µ–Ω CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"-->–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...\n",
      "üîÑ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "<|im_start|>user\n",
      "–°–æ—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ –∏–∑ –¥–µ—Å—è—Ç–∏ –ø—Ä–µ–¥–º–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —á–µ–ª–æ–≤–µ–∫—É –≤ –ø–æ—Ö–æ–¥–µ.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "–í–æ—Ç –¥–µ—Å—è—Ç—å –ø—Ä–µ–¥–º–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —á–µ–ª–æ–≤–µ–∫—É –≤ –ø–æ—Ö–æ–¥–µ:\n",
      "\n",
      "1. –ü–∞–ª–∞—Ç–∫–∞ - –¥–ª—è —É–∫—Ä—ã—Ç–∏—è –∏ –∑–∞—â–∏—Ç—ã –æ—Ç –Ω–µ–ø–æ–≥–æ–¥—ã.\n",
      "2. –°–ø–∞–ª—å–Ω—ã–π –º–µ—à–æ–∫ - —á—Ç–æ–±—ã —Å–ø–∞—Ç—å –±—ã–ª–æ —Ç–µ–ø–ª–æ –∏ –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ\n",
      "3. –ü–µ—Ä–µ–Ω–æ—Å–Ω–∞—è –ø–ª–∏—Ç–∞ –∏–ª–∏ –≥—Ä–∏–ª—å –¥–ª—è –∫–æ—Å—Ç—Ä–∞ ‚Äì –¥–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –ø–∏—â–∏.\n",
      "4. –û—Ö–ª–∞–¥–∏—Ç–µ–ª—å —Å–æ –ª—å–¥–æ–º –∏–ª–∏ –ø–∞–∫–µ—Ç–∞–º–∏ —Å–æ –ª—å–¥–æ–º - —á—Ç–æ–±—ã —Å–∫–æ—Ä–æ–ø–æ—Ä—Ç—è—â–∏–µ—Å—è –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –Ω–∞–ø–∏—Ç–∫–∏ –æ—Å—Ç–∞–≤–∞–ª–∏—Å—å —Ö–æ–ª–æ–¥–Ω—ã–º–∏.\n",
      "5. –§–æ–Ω–∞—Ä—å –∏–ª–∏ —Ñ–æ–Ω–∞—Ä...\n",
      "\n",
      "‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: 51760 –ø—Ä–∏–º–µ—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# 2: \n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "def load_and_format_dataset():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\"\"\"\n",
    "    \n",
    "    print(\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...\")\n",
    "    ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "    \n",
    "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "    def format_alpaca_to_chat(example):\n",
    "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä Alpaca –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Qwen\"\"\"\n",
    "        if example['input']:\n",
    "            user_message = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
    "        else:\n",
    "            user_message = example['instruction']\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        \n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ Qwen\n",
    "        formatted_text = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                formatted_text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n",
    "            else:\n",
    "                formatted_text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n",
    "        \n",
    "        return {\"text\": formatted_text}\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "    print(\"üîÑ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
    "    formatted_ds = ds.map(format_alpaca_to_chat)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ\n",
    "    print(\"\\n–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "    print(formatted_ds[11]['text'][:500] + \"...\")\n",
    "    print(f\"\\n‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(formatted_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    \n",
    "    return formatted_ds\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "formatted_ds = load_and_format_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –î–û –æ–±—É—á–µ–Ω–∏—è...\n",
      "\n",
      "1. –í–æ–ø—Ä–æ—Å: —Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n",
      "–û—Ç–≤–µ—Ç: —Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?Áéì\n",
      " Comey\n",
      "—à–µ—Å—Ç—å Comey\n",
      " Comey\n",
      "—à–µ—Å—Ç—å Comey\n",
      " Comey\n",
      "—à–µ—Å—Ç—å Comey\n",
      " Comey\n",
      "—à–µ—Å—Ç—å Comey\n",
      "\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      "\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "2. –í–æ–ø—Ä–æ—Å: —Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\n",
      "–û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –∏–∑ –º–æ–µ–≥–æ –Ω–∞–±–æ—Ä–∞.\n",
      " Crimea\n",
      " Crimea, –Ω–∞ –¥–Ω–µ –í–æ–ª–≥–∏\n",
      " –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–∏—à–∏ –∏ –ø—É—Å—Ç–æ—Ç—ã\n",
      " –û—Å–æ–±—ã–º –ø–ª–µ–º–µ–Ω–µ–º\n",
      " –û—Å–∞–ø–∞\n",
      " –û—Å–∞–ø–∞, –Ω–∞ –¥–Ω–µ –í\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "3. –í–æ–ø—Ä–æ—Å: –∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
      "–û—Ç–≤–µ—Ç: –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –∫–∏—Å—Ç—å –∏ –ø–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –∫—Ä—ã–∂–æ–≤–Ω–∏–∫–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å.\n",
      "\n",
      "mite\n",
      "–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?mite\n",
      " Comey\n",
      "–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫? Comey\n",
      " Comey\n",
      "–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "4. –í–æ–ø—Ä–æ—Å: –ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\n",
      "–û—Ç–≤–µ—Ç: –ß—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ —É–ø–æ—Ä—è–¥–æ—á–∏—Ç—å –∑–Ω–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥—ã –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è. createStackNavigator()\n",
      " createStackNavigator() - —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ –∫–ª–∞—Å—Å–∞ createStackNavigator –≤ –≤–∞—à–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.\n",
      " createStackNavigator() - —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n"
     ]
    }
   ],
   "source": [
    "# 3: \n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –î–û –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def test_before_training():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    print(\"–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –î–û –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    questions_rus = [\n",
    "        \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "        \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\",\n",
    "        \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "        \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(questions_rus, 1):\n",
    "        print(f\"\\n{i}. –í–æ–ø—Ä–æ—Å: {question}\")\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"–û—Ç–≤–µ—Ç: {answer}\")\n",
    "        print(\"‚Äï\" * 55)\n",
    "\n",
    "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "test_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen/Qwen2.5-0.5B...\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n"
     ]
    }
   ],
   "source": [
    "# 4: \n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SFT —Å LoRA\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "def setup_training():\n",
    "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å SFTConfig\"\"\"\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "    print(f\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=8, \n",
    "        lora_alpha=16, \n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º SFTConfig \n",
    "    cfg = SFTConfig(\n",
    "        output_dir=\"./qwen2.5-0.5b-sft-ru\",\n",
    "        dataset_text_field=\"text\",\n",
    "        #max_seq_length=1024,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  \n",
    "        logging_steps=50,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        report_to=None,\n",
    "        run_name='qwen-lora-sft',\n",
    "        save_steps=500,\n",
    "        fp16=True,  # –í–∫–ª—é—á–∞–µ–º mixed precision –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "        optim=\"adamw_torch\",\n",
    "        remove_unused_columns=False,\n",
    "        packing=True,  # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, peft_cfg, cfg\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "model, tokenizer, peft_cfg, cfg = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51760/51760 [00:57<00:00, 902.98 examples/s] \n",
      "Packing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51760/51760 [00:00<00:00, 64309.12 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1697' max='1697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1697/1697 1:27:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.915500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.601700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.574100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...\n",
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "# 5: –ó–∞–ø—É—Å–∫ SFT –æ–±—É—á–µ–Ω–∏—è —Å LoRA\n",
    "def create_and_train_model(formatted_ds):\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å SFTConfig\"\"\"\n",
    "    \n",
    "    print(\"üîÑ –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä...\")\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º processing_class\n",
    "        args=cfg,\n",
    "        train_dataset=formatted_ds,\n",
    "        peft_config=peft_cfg,\n",
    "        #dataset_text_field=\"text\",  # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø–æ–ª—è\n",
    "        #max_seq_length=1024,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º max_seq_length\n",
    "        #packing=True,  # –í–∫–ª—é—á–∞–µ–º packing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    )\n",
    "    \n",
    "    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "    print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(\"./qwen2.5-0.5b-sft-ru\")\n",
    "    \n",
    "    print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "    return trainer\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "trainer = create_and_train_model(formatted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ubuntu/.venv/lib/python3.10/site-packages (4.67.1)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting notebook (from jupyter)\n",
      "  Using cached notebook-7.5.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyter) (7.1.0)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Using cached jupyterlab-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.venv/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (5.9.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from ipykernel->jupyter) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter) (1.17.0)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
      "  Using cached jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
      "  Using cached jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab->jupyter)\n",
      "  Using cached jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter)\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyterlab->jupyter) (59.6.0)\n",
      "Collecting tomli>=1.2.2 (from jupyterlab->jupyter)\n",
      "  Using cached tomli-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
      "  Using cached json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: requests>=2.31 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2.32.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (25.4.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
      "  Using cached rpds_py-0.29.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.3)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached webcolors-25.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter)\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached bleach-6.3.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)\n",
      "  Using cached mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter)\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2.5.0)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached lark-1.3.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached arrow-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tzdata in /home/ubuntu/.venv/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2025.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/ubuntu/.venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached jupyterlab-4.5.0-py3-none-any.whl (12.4 MB)\n",
      "Using cached jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Using cached jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
      "Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "Using cached python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Using cached lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "Using cached rpds_py-0.29.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (392 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "Using cached webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (87 kB)\n",
      "Using cached cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
      "Using cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.4.0-py3-none-any.whl (68 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached notebook-7.5.0-py3-none-any.whl (14.5 MB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, fastjsonschema, widgetsnbextension, websocket-client, webcolors, uri-template, tomli, tinycss2, terminado, soupsieve, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, python-json-logger, pycparser, prometheus-client, pandocfilters, overrides, mistune, lark, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, fqdn, defusedxml, bleach, babel, async-lru, rfc3987-syntax, referencing, jupyter-server-terminals, cffi, beautifulsoup4, arrow, jsonschema-specifications, isoduration, ipywidgets, argon2-cffi-bindings, jupyter-console, jsonschema, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54/54\u001b[0m [jupyter]2/54\u001b[0m [notebook]b]]er]t]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.4.0 async-lru-2.0.5 babel-2.17.0 beautifulsoup4-4.14.2 bleach-6.3.0 cffi-2.0.0 defusedxml-0.7.1 fastjsonschema-2.21.2 fqdn-1.5.1 ipywidgets-8.1.8 isoduration-20.11.0 json5-0.12.1 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.5.0 jupyterlab-pygments-0.3.0 jupyterlab-server-2.28.0 jupyterlab_widgets-3.0.16 lark-1.3.1 mistune-3.1.4 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.5.0 notebook-shim-0.2.4 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.23.1 pycparser-2.23 python-json-logger-4.0.0 referencing-0.37.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.29.0 send2trash-1.8.3 soupsieve-2.8 terminado-0.18.1 tinycss2-1.4.0 tomli-2.3.0 uri-template-1.3.0 webcolors-25.10.0 webencodings-0.5.1 websocket-client-1.9.0 widgetsnbextension-4.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tqdm ipywidgets jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è...\n",
      "–û—Ç–≤–µ—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\n",
      "\n",
      "\n",
      "1. –í–æ–ø—Ä–æ—Å: —Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n",
      "–û—Ç–≤–µ—Ç: –°–æ–ª–Ω–µ—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 8 —Å–æ–ª–Ω–µ—á–Ω—ã—ÖË°åÊòü, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Å–æ–ª–Ω—Ü–µ–º –∏ –æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–≤–µ—Ç. –û–Ω–∏ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è: –ú–∞—Ä—Å, –í–µ–Ω–µ—Ä–∞, –Æ–ø–∏—Ç–µ—Ä, –°–∞—Ç—É—Ä–Ω, –ì–∞–ª–∞–∫—Ç–∏–∫–∞, –ó–µ–º–ª—è, –ú–∞—Ä—Å –∏ –°–æ–ª–Ω—Ü–µ. –°–æ–ª–Ω—Ü–µ, –∫–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω–æ, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ü–µ–Ω—Ç—Ä–µ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–ª–Ω–µ—á–Ω—ã–µË°åÊòü –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö –æ—Ç –Ω–µ–≥–æ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å–æ–ª–Ω–µ—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞—é—Ç —Ç–µ–Ω–µ–≤–æ–π –∏ –Ω–µ–Ω–µ–Ω–µ—Ñ–µ—Ç—Å–∫–∏–µ —Å–æ–ª–Ω–µ—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. –û–¥–Ω–∞–∫–æ, —Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ü–µ–Ω–∫–∞–º, –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –æ–∫–æ–ª–æ 1700 –Ω–µ–Ω–µ–Ω–µ—Ñ–µ—Ç—Å–∫–∏—Ö —Å–æ–ª–Ω–µ—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö —è–≤–ª—è—é—Ç—Å—è –º–µ–Ω–µ–µ —Ä–∞–∑–≤–∏—Ç—ã–º–∏, –∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª–µ–µ —Ä–∞–∑–≤–∏—Ç—ã–º–∏, —á–µ–º –≤ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ. –°–æ–ª–Ω–µ—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 8 —Å–æ–ª–Ω–µ—á–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç, –≤–∫–ª—é—á–∞—è –ú–∞—Ä—Å, –í–µ–Ω–µ—Ä—É, –Æ–ø–∏—Ç–µ—Ä, –°–∞—Ç—É—Ä–Ω, –ì–∞–ª–∞–∫—Ç–∏–∫—É, –ó–µ–º–ª—é, –ú–∞—Ä\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "2. –í–æ–ø—Ä–æ—Å: —Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\n",
      "–û—Ç–≤–µ—Ç: –û–¥–Ω–∞ —É—Ç–∫–∞, –Ω–∞–∫–∞—Ç–∞–≤—à–∞—è—Å—è –Ω–∞ –∑–µ–º–ª—é, –ø—ã—Ç–∞–ª–∞—Å—å —Å–ø—Ä—è—Ç–∞—Ç—å—Å—è –≤ —É–≥–ª—É, –Ω–æ —É–≤–∏–¥–µ–≤, —á—Ç–æ —É –Ω–µ–µ –µ—Å—Ç—å –≤—Ä–∞–≥, –æ–Ω–∞ –ø–æ–¥–±–µ–∂–∞–ª–∞ –∫ –Ω–µ–º—É –∏, –∫–æ–≥–¥–∞ –æ–Ω–∞ —Å–µ–ª–∞ –Ω–∞ –Ω–µ–≥–æ, –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞–ª–∞, –∫–∞–∫ –æ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç –¥–≤–∏–≥–∞—Ç—å—Å—è –≤–ø–µ—Ä–µ–¥. –û–Ω–∞ —Å–Ω—è–ª–∞ –∫—Ä—é—á–æ–∫, —á—Ç–æ–±—ã —Å–Ω—è—Ç—å —Å –Ω–µ–≥–æ —É—à–∏ –∏ –Ω–∞–∫–æ–Ω–µ—Ü –ø–æ–¥–æ—à–ª–∞ –∫ –Ω–µ–π. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ –≤ —Ç–æ–ª–ø–µ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞ —Ä—è–¥–æ–º –∏ –ø—Ä–∏–Ω—è–ª–∞—Å—å –≥—Ä—ã–∑—Ç—å –µ–≥–æ, –ø–æ–∫–∞ –æ–Ω –Ω–µ —Å–ø—É—Å—Ç–∏–ª—Å—è. –û–Ω–∞ —Å–µ–ª–∞\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "3. –í–æ–ø—Ä–æ—Å: –∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
      "–û—Ç–≤–µ—Ç: –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã —Å–æ–±—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π –∞–π—Ñ–æ—Ç–µ–∫—É –∏–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–∏—è. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–∏—è –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é —Ñ—É–Ω–∫—Ü–∏—é. –í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–∏—è, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–Ω—ã–π –∫–∏—Å—Ç—å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞–º–∏, –µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –µ—Å—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã.·®®\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "4. –í–æ–ø—Ä–æ—Å: –ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\n",
      "–û—Ç–≤–µ—Ç: –°–æ–≤–µ—Ç—É—é –Ω–∞—á–∞—Ç—å —Å –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ Rosetta Stone –∏–ª–∏ FluentU. –≠—Ç–æ –ø–æ–º–æ–≥—É—Ç –≤–∞–º –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ —è–∑—ã–∫–∞ –∏ —Å–æ–∑–¥–∞—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏ –ø—Ä–∞–≤–∏–ª. –ù–∞–¥–æ —Ç–∞–∫–∂–µ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ–±—ã —É–º–µ—Ç—å –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å–ª–æ–≤–∞ –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤—ã—Ä–∞–∑–∏—Ç—å —Å–≤–æ–∏ –º—ã—Å–ª–∏ –∏ –∂–µ–ª–∞–Ω–∏—è. –ü–æ–≤—Ç–æ—Ä—è–π—Ç–µ —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –∏ –ø–æ–≤—Ç–æ—Ä—è–π—Ç–µ —Å–ª–æ–≤–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ —É—Ç—Ä–∞—Ç–∏–ª–∏ —Å–≤–æ–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏. –ù–µ —Ç–æ—Ä–æ–ø–∏—Ç–µ—Å—å, –∏–±–æ —É –≤–∞—Å –µ—Å—Ç—å –≤—Ä–µ–º—è –∏ —Å–∏–ª—ã, —á—Ç–æ–±—ã –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏ –Ω–∞–≤—ã–∫–∏ —è–∑—ã–∫–∞. –ò—Ç–∞–∫, –Ω–∞—á–Ω–∏—Ç–µ —Å –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–∞, –∏ –±—É–¥—å—Ç–µ patient–∏, –∏ –±—É–¥–µ—Ç–µ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ —è–∑—ã–∫–∞ –∏ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ –∏ –ø—Ä–∞–≤–∏–ª. –ê –ø–æ—Ç–æ–º, –∫–æ–≥–¥–∞ –≤—ã –±—É–¥–µ—Ç–µ –≥–æ—Ç–æ–≤—ã, –Ω–∞—á–Ω–∏—Ç–µ –∑–∞–Ω—è—Ç–∏—è —Å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ –∏ —Å–ª–æ–∂–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ù–æ –±—É–¥—å—Ç–µ patient–∏ –∏ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ—Å—å —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–µ —Ç–æ—Ä\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n"
     ]
    }
   ],
   "source": [
    "#  6: (–≤—Ç–æ—Ä–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def test_after_training():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    print(\"\\n–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model_path = \"./qwen2.5-0.5b-sft-ru\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    questions_rus = [\n",
    "        \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "        \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\",\n",
    "        \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "        \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"–û—Ç–≤–µ—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\\n\")\n",
    "    \n",
    "    for i, question in enumerate(questions_rus, 1):\n",
    "        print(f\"\\n{i}. –í–æ–ø—Ä–æ—Å: {question}\")\n",
    "        \n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º system –ø—Ä–æ–º–ø—Ç –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç\n",
    "        response = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"–û—Ç–≤–µ—Ç: {answer.strip()}\")\n",
    "        print(\"‚Äï\" * 55)\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "test_after_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏\n",
    "def cleanup_memory():\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU\"\"\"\n",
    "    import gc\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n",
    "\n",
    "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
