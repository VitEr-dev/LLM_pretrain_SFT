{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM: post-train SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: datasets in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: trl in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: tqdm in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: accelerate in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: setuptools in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.11)\n",
      "Requirement already satisfied: psutil in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vitaliy/myproj/venv_3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch datasets trl tqdm accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ SFT –æ–±—É—á–µ–Ω–∏—è Qwen/Qwen2.5-0.5B –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–º –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ d0rj/alpaca-cleaned-ru –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\n",
      "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ:\n",
      "-->–î–æ—Å—Ç—É–ø–µ–Ω CUDA: False\n",
      "-->–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\n"
     ]
    }
   ],
   "source": [
    "#  1 :\n",
    "# –∏–º–ø–æ—Ä—Ç—ã –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "print(\"–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\")\n",
    "\n",
    "# –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ:\")\n",
    "print(f\"-->–î–æ—Å—Ç—É–ø–µ–Ω CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"-->–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...\n",
      "üîÑ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...\n",
      "\n",
      " –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "<|im_start|>user\n",
      "–°–æ—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ –∏–∑ –¥–µ—Å—è—Ç–∏ –ø—Ä–µ–¥–º–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —á–µ–ª–æ–≤–µ–∫—É –≤ –ø–æ—Ö–æ–¥–µ.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "–í–æ—Ç –¥–µ—Å—è—Ç—å –ø—Ä–µ–¥–º–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —á–µ–ª–æ–≤–µ–∫—É –≤ –ø–æ—Ö–æ–¥–µ:\n",
      "\n",
      "1. –ü–∞–ª–∞—Ç–∫–∞ - –¥–ª—è —É–∫—Ä—ã—Ç–∏—è –∏ –∑–∞—â–∏—Ç—ã –æ—Ç –Ω–µ–ø–æ–≥–æ–¥—ã.\n",
      "2. –°–ø–∞–ª—å–Ω—ã–π –º–µ—à–æ–∫ - —á—Ç–æ–±—ã —Å–ø–∞—Ç—å –±—ã–ª–æ —Ç–µ–ø–ª–æ –∏ –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ\n",
      "3. –ü–µ—Ä–µ–Ω–æ—Å–Ω–∞—è –ø–ª–∏—Ç–∞ –∏–ª–∏ –≥—Ä–∏–ª—å –¥–ª—è –∫–æ—Å—Ç—Ä–∞ ‚Äì –¥–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –ø–∏—â–∏.\n",
      "4. –û—Ö–ª–∞–¥–∏—Ç–µ–ª—å —Å–æ –ª—å–¥–æ–º –∏–ª–∏ –ø–∞–∫–µ—Ç–∞–º–∏ —Å–æ –ª—å–¥–æ–º - —á—Ç–æ–±—ã —Å–∫–æ—Ä–æ–ø–æ—Ä—Ç—è—â–∏–µ—Å—è –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –Ω–∞–ø–∏—Ç–∫–∏ –æ—Å—Ç–∞–≤–∞–ª–∏—Å—å —Ö–æ–ª–æ–¥–Ω—ã–º–∏.\n",
      "5. –§–æ–Ω–∞—Ä—å –∏–ª–∏ —Ñ–æ–Ω–∞—Ä...\n",
      "\n",
      " ‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: 51760 –ø—Ä–∏–º–µ—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "#   2 : \n",
    "# –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "def load_and_format_dataset():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\"\"\"\n",
    "    \n",
    "    print(\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...\")\n",
    "    ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "    \n",
    "    # —Ña—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "    def format_alpaca_to_chat(example):\n",
    "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä Alpaca –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Qwen\"\"\"\n",
    "        if example['input']:\n",
    "            user_message = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
    "        else:\n",
    "            user_message = example['instruction']\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        \n",
    "        # —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ Qwen\n",
    "        formatted_text = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                formatted_text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n",
    "            else:\n",
    "                formatted_text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n",
    "        \n",
    "        return {\"text\": formatted_text}\n",
    "    \n",
    "    # –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "    print(\"üîÑ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
    "    formatted_ds = ds.map(format_alpaca_to_chat)\n",
    "    \n",
    "    # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ\n",
    "    print(\"\\n –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "    print(formatted_ds[11]['text'][:500] + \"...\")\n",
    "    print(f\"\\n ‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(formatted_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    \n",
    "    return formatted_ds\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "formatted_ds = load_and_format_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –î–û –æ–±—É—á–µ–Ω–∏—è...\n",
      "\n",
      "1. –í–æ–ø—Ä–æ—Å: —Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n",
      " –û—Ç–≤–µ—Ç: –í –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –µ—Å—Ç—å 8 –ø–ª–∞–Ω–µ—Ç.\n",
      "—ê\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?—ê\n",
      "\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "2. –í–æ–ø—Ä–æ—Å: —Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\n",
      " –û—Ç–≤–µ—Ç: –ß—Ç–æ —Ç–∞–∫–æ–µ —Å—Ç–∏—Ö coma?\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      " Crimea\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "3. –í–æ–ø—Ä–æ—Å: –∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
      " –û—Ç–≤–µ—Ç: –ö–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –∫—Ä—ã–∂–æ–≤–Ω–∏–∫, –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –µ–≥–æ –≤ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –∏ —Ä–∞–∑–º–µ—Ä—ã. –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∫—Ä—ã\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
      "\n",
      "4. –í–æ–ø—Ä–æ—Å: –ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\n",
      " –û—Ç–≤–µ—Ç: –ß—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –Ω–∞—É—á–∏—Ç—å—Å—è –Ω–æ–≤–æ–º—É —è–∑—ã–∫—É, –≤–∞–º –Ω—É–∂–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞–º –∏ –ø—Ä–∞–∫—Ç–∏–∫—É–º.Ê∂û\n",
      "—ê\n",
      "–ß—Ç–æ\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n"
     ]
    }
   ],
   "source": [
    "#   3 : \n",
    "# —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –î–û –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def test_before_training():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    print(\"–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –î–û –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    \n",
    "    questions_rus = [\n",
    "        \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "        \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\",\n",
    "        \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "        \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(questions_rus, 1):\n",
    "        print(f\"\\n{i}. –í–æ–ø—Ä–æ—Å: {question}\")\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\" –û—Ç–≤–µ—Ç: {answer}\")\n",
    "        print(\"‚Äï\" * 55)\n",
    "\n",
    "# –∑–∞–ø—É—Å—Ç–∏—Ç—å –¥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "test_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  4 : \n",
    "# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ SFT –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def setup_training():\n",
    "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "    print(f\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞)\n",
    "    if device == \"cuda\":\n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPU T4 16GB\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./qwen2.5-0.5b-sft-ru\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-5,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.03,\n",
    "            max_grad_norm=0.3,\n",
    "            logging_steps=10,\n",
    "            save_steps=500,\n",
    "            eval_steps=500,\n",
    "            save_total_limit=2,\n",
    "            remove_unused_columns=False,\n",
    "            push_to_hub=False,\n",
    "            report_to=None,\n",
    "            dataloader_pin_memory=False,\n",
    "            ddp_find_unused_parameters=False,\n",
    "        )\n",
    "    else:\n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è CPU Mac M2\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./qwen2.5-0.5b-sft-ru\",\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-5,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.03,\n",
    "            max_grad_norm=0.3,\n",
    "            logging_steps=5,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            remove_unused_columns=False,\n",
    "            push_to_hub=False,\n",
    "            report_to=None,\n",
    "        )\n",
    "    \n",
    "    return model, tokenizer, training_args\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "model, tokenizer, training_args = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  5 : \n",
    "# –ó–∞–ø—É—Å–∫ SFT –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def create_and_train_model(formatted_ds):\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    \n",
    "    print(\"üîÑ –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä...\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=formatted_ds,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=1024,\n",
    "        packing=False,  \n",
    "    )\n",
    "    \n",
    "    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "    print(\"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(\"./qwen2.5-0.5b-sft-ru\")\n",
    "    \n",
    "    print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "    return trainer\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "trainer = create_and_train_model(formatted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  6 : \n",
    "# —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "def test_after_training():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    print(\"\\n –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ü–û–°–õ–ï –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model_path = \"./qwen2.5-0.5b-sft-ru\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    questions_rus = [\n",
    "        \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "        \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\",\n",
    "        \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "        \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "    ]\n",
    "    \n",
    "    print(\" –û—Ç–≤–µ—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\\n\")\n",
    "    \n",
    "    for i, question in enumerate(tqdm(questions_rus, desc=\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\"), 1):\n",
    "        print(f\"\\n{i}. –í–æ–ø—Ä–æ—Å: {question}\")\n",
    "        \n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ\n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç\n",
    "        response = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\" –û—Ç–≤–µ—Ç: {answer.strip()}\")\n",
    "        print(\"‚Äï\" * 55)\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "test_after_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  7 : \n",
    "# –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ (–µ—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è, —Ä–∞—Å–∫–æ–º–∏—Ç–∏—Ç—å)\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU\"\"\"\n",
    "    import gc\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n",
    "\n",
    "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "# cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
