{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM: pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/.venv/lib/python3.10/site-packages (4.57.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.venv/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: tokenizers in /home/ubuntu/.venv/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.venv/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.venv/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: trl in /home/ubuntu/.venv/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.venv/lib/python3.10/site-packages (4.67.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/ubuntu/.venv/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.venv/lib/python3.10/site-packages (from accelerate) (7.1.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]7\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch datasets tokenizers accelerate numpy torch trl tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# 1:\n",
    "# –ò–º–ø–æ—Ä—Ç—ã\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer, \n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    GenerationConfig\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "print(\"–ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ ‚úÖ\n",
      "–ü–∞–ø–∫–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö: ./data\n",
      "–ü–∞–ø–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏: ./llm_model\n",
      "–ü–∞–ø–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞: ./tokenizer\n"
     ]
    }
   ],
   "source": [
    "# 2: \n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = \"./data\"  # –∑–¥–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç - .txt —Ñ–∞–π–ª—ã\n",
    "    VOCAB_SIZE = 3000\n",
    "    CONTEXT_LENGTH = 512\n",
    "    MODEL_SAVE_PATH = \"./llm_model\"\n",
    "    TOKENIZER_SAVE_PATH = \"./tokenizer\"\n",
    "    \n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "    HIDDEN_SIZE = 1024\n",
    "    INTERMEDIATE_SIZE = 1536\n",
    "    NUM_HIDDEN_LAYERS = 16\n",
    "    NUM_ATTENTION_HEADS = 16\n",
    "    NUM_KEY_VALUE_HEADS = 8\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã\n",
    "    TEST_PROMPTS = [\n",
    "        \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\",\n",
    "        \"–°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\",\n",
    "        \"–ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\",\n",
    "        \"–ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\",\n",
    "        \"–ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\",\n",
    "        \"–õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\",\n",
    "        \"–ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\",\n",
    "        \"–í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\",\n",
    "        \"–í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\",\n",
    "        \"–ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\"\n",
    "    ]\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–ø–æ–∫\n",
    "os.makedirs(Config.DATA_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MODEL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(Config.TOKENIZER_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "print(\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ ‚úÖ\")\n",
    "print(f\"–ü–∞–ø–∫–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö: {Config.DATA_DIR}\")\n",
    "print(f\"–ü–∞–ø–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏: {Config.MODEL_SAVE_PATH}\")\n",
    "print(f\"–ü–∞–ø–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞: {Config.TOKENIZER_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π ...\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Pushkin_CapitanskaaDochka.txt (214409 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Sologub_KorolevaOrtruda.txt (557621 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Chekhov_Dama.txt (335586 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gogol_Viy.txt (75506 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_ZyznKlimaSamgina3.txt (692089 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Zhukova_Dacha.txt (166564 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Otchayanie_1934.txt (276782 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: NKhvoshchinskaya_PervayaBorba.txt (359727 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Veschi_1972_Ilin.txt (165577 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Sologub_TjazolyjeSny.txt (640729 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_SemeynoyeSchastiye.txt (165071 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Bulgakov_BelayaGvardiya.txt (504043 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Zashchita_1930.txt (348927 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gippius_Roman-Tzarevich.txt (426568 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Panaeva_Paseka.txt (169231 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevsky_Karamazow1.txt (378137 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_FomaGordeev.txt (502033 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: NKhvoshchinskaya_Bratets.txt (120402 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: NKhvoshchinskaya_PoslijePotopa.txt (32574 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevsky_PrestupleniyeINakazaniye.txt (1094323 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Volkonskaya_Olga2-6.txt (85330 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Bulgakov_RokovyeYaytsa.txt (147697 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_ZyznKlimaSamgina1.txt (978993 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevski_Biesy.txt (1283887 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_Zapiski prichyotnika.txt (631351 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_Toy.txt (81655 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Panaeva_Domashny.txt (172189 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevsky_Karamazow2.txt (397876 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_ZyznKlimaSamgina4.txt (903795 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Pushkin_ArapPetraVelikogo.txt (62126 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_Zateinik.txt (39602 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Pnin_1957_Barabtarlo.txt (295728 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_ArtamonovBusiness.txt (482653 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Kazn_1936.txt (286132 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_PuteshestvieVoVnutrStrany.txt (65875 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gippius_ChertovayaKukla.txt (316380 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_Chelovek.txt (128103 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tur_Starushka.txt (304572 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_Pavilion.txt (219202 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: NKhvoshchinskaya_Pansionerka.txt (195076 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Pushkin_Dubrovsky.txt (138677 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gogol_StarosvyetskiyePomeshchiki.txt (46763 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_VoynaIMir_ver1.txt (1161090 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_Voskreseniye.txt (911476 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Bulgakov_Master.txt (775109 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Zhukova_BaronReichman.txt (66413 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Sholokhov_TikhiiDon.txt (3075805 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_Nakanune.txt (287445 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Pnin_1957_Ilin.txt (300391 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gan_SudSveta.txt (144596 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gan_Ideal.txt (99232 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Krjukov_Shkval.txt (161074 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Bulgakov_TeatralnyjRoman.txt (294301 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_VoynaIMir1.txt (791558 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Panaeva_Talnikov.txt (221466 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_ZapiskiPrichyotnika.txt (631351 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Zinovieva-Annibal_TridtsatTriUgoda.txt (45087 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_AnnaKarenina.txt (1735653 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Bulgakov_Diavoliada.txt (70681 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Arlekinov_1974_Ilin.txt (409064 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_Ugol.txt (187312 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Podvig_1932.txt (327801 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Pnin_1957_Nosik.txt (319791 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: SKhvoshchinskaya_GorodskieIDerevenskie.txt (292891 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Bulgakov_ZapiskiYonogoVracha.txt (228676 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_Dym.txt (326776 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Kamera_1933.txt (290668 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_IgraSudby.txt (126320 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevsky_Karamazow3.txt (433092 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: NKhvoshchinskaya_Proshaniye.txt (105568 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_Yurchak.txt (299254 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Krjukov_NaTichomDonu.txt (207496 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Korol_1928.txt (370616 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Pushkin_Pikovaya.txt (47443 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_ChervonnyjKorol.txt (44850 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_Nov.txt (511997 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_VeshniyeVody.txt (259483 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevsky_BednyeLyudi.txt (254032 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Sologub_MelkijBes.txt (540479 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Mashenka_1926.txt (168330 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gogol_Mertvye.txt (496806 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_ZyznKlimaSamgina2.txt (1182600 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_VoynaIMir4.txt (479906 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Chekhov_Dyadya.txt (85151 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_SernyjKljuch.txt (57277 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_GodZyznijWPeterburge.txt (116506 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Sholokhov_PodnyatayaTselina.txt (1404270 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Dar_1938.txt (718076 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Panaeva_Steppe.txt (147037 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevsky_Karamazow4.txt (628084 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gorky_Mat.txt (580416 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Nait_1941_Ilin.txt (325441 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Pushkin_PovestiPokoynogoIvanaPetrovichaBelkina.txt (124837 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Durova_Gudishki.txt (487110 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Vovchok_TulevayaBaba.txt (58489 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Ada_1969_Ilin.txt (1118606 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Dostoyevski_Idiot.txt (1328735 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_VoynaIMir2.txt (832104 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Sologub_KapliKrovi.txt (393222 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_DvoryanskoyeGnezdo.txt (310739 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_OttsyIDeti.txt (366219 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Krjukov_Zyb.txt (149432 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Gogol_Taras.txt (495738 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Turgenev_Rudin.txt (236451 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Lolita_1955_Nabokov.txt (731217 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tur_ShalonskiFamily.txt (202124 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Nabokov_Znakom_1947_Ilin.txt (390275 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "- –ó–∞–≥—Ä—É–∂–µ–Ω: Tolstoi_VoynaIMir3.txt (887092 —Å–∏–º–≤–æ–ª–æ–≤)\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 108 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "\n",
      "üîÑ–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\n",
      "‚úÖ –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 107 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "\n",
      "üîÑ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\n",
      "‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 107/107 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: 107 —Ç–µ–∫—Å—Ç–æ–≤\n",
      " \n",
      "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: 399716 —Å–∏–º–≤–æ–ª–æ–≤\n",
      " –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 31094 —Å–∏–º–≤–æ–ª–æ–≤\n",
      " –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 2882086 —Å–∏–º–≤–æ–ª–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# 3: \n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞, –æ—á–∏—Å—Ç–∫–∞, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "    \n",
    "    def load_texts(self) -> List[str]:\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö .txt —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "        texts = []\n",
    "        for filename in os.listdir(self.config.DATA_DIR):\n",
    "            if filename.endswith('.txt'):\n",
    "                filepath = os.path.join(self.config.DATA_DIR, filename)\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read().strip()\n",
    "                        if content:\n",
    "                            texts.append(content)\n",
    "                            print(f\"- –ó–∞–≥—Ä—É–∂–µ–Ω: {filename} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}\")\n",
    "        return texts\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    \n",
    "        # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã (–∏ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –∏ —Å—Ç—Ä–æ—á–Ω—ã–µ)\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º: –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã (–≤—Å–µ —Ä–µ–≥–∏—Å—Ç—Ä—ã), –ø—Ä–æ–±–µ–ª—ã, —Ä—É—Å—Å–∫—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "        text = re.sub(r'[^–∞-—è—ë–ê-–Ø–Å\\s\\.,!?;:‚Äî\\-()¬´¬ª]', '', text)\n",
    "    \n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
    "        text = re.sub(r'[\\.]{2,}', '.', text)      # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏\n",
    "        text = re.sub(r'[,]{2,}', ',', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ\n",
    "        text = re.sub(r'[!]{2,}', '!', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è\n",
    "        text = re.sub(r'[?]{2,}', '?', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n",
    "        text = re.sub(r'[;]{2,}', ';', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π\n",
    "        text = re.sub(r'[:]{2,}', ':', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–≤–æ–µ—Ç–æ—á–∏—è\n",
    "        text = re.sub(r'[-]{2,}', '-', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã/—Ç–∏—Ä–µ\n",
    "        text = re.sub(r'[‚Äî]{2,}', '‚Äî', text)       # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–∏—Ä–µ\n",
    "    \n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
    "        text = re.sub(r'[!?]+', '!', text)         # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ ! –∏ ? –≤ –ª—é–±—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—è—Ö\n",
    "        text = re.sub(r'[\\.!?]+', '.', text)       # –ª—é–±—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–æ—á–µ–∫, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π, –≤–æ–ø—Ä–æ—Å–æ–≤\n",
    "    \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã (—É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –ø—Ä–æ–±–µ–ª–æ–≤)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä - –í–°–ï –±—É–∫–≤—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —Å—Ç—Ä–æ—á–Ω—ã–º–∏\n",
    "        text = text.lower()\n",
    "    \n",
    "        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
    "        text = re.sub(r'\\s+([\\.,!?;:])', r'\\1', text)\n",
    "    \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç\n",
    "        text = re.sub(r'([\\.,!?;:])([–∞-—è—ë])', r'\\1 \\2', text)\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    def remove_duplicates(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\"\"\"\n",
    "        seen = set()\n",
    "        unique_texts = []\n",
    "        for text in texts:\n",
    "            text_hash = hash(text[:1000])\n",
    "            if text_hash not in seen:\n",
    "                seen.add(text_hash)\n",
    "                unique_texts.append(text)\n",
    "        return unique_texts\n",
    "    \n",
    "    def process_data(self) -> List[str]:\n",
    "        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
    "        print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π ...\")\n",
    "        texts = self.load_texts()\n",
    "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "        \n",
    "        if not texts:\n",
    "            raise ValueError(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–ø–∫–µ data/\")\n",
    "        \n",
    "        print(\"\\nüîÑ–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\")\n",
    "        texts = self.remove_duplicates(texts)\n",
    "        print(f\"‚úÖ –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "        \n",
    "        print(\"\\nüîÑ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
    "        cleaned_texts = []\n",
    "        for i, text in enumerate(texts):\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            if cleaned_text:\n",
    "                cleaned_texts.append(cleaned_text)\n",
    "            if (i + 1) % 107 == 0:\n",
    "                print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "                print(f\"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        lengths = [len(text) for text in cleaned_texts]\n",
    "        print(f\" \\n–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {np.mean(lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        print(f\" –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {min(lengths)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        print(f\" –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max(lengths)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        \n",
    "        return cleaned_texts\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "processor = DataProcessor()\n",
    "cleaned_texts = processor.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " –ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "-------------------------------------------------------\n",
      "–¥–µ–∫–∞–±—Ä—è. —Å–µ–≥–æ–¥–Ω—è —è –ø—Ä–æ—Å–Ω—É–ª–∞—Å—å –æ—á–µ–Ω—å —Ä–∞–Ω–æ. –≥–æ—Ä–µ–ª–∞ —Å–≤–µ—á–∞ –Ω–∞ –º–æ–µ–º —Å—Ç–æ–ª–∏–∫–µ, —É –º–æ–µ–π –ø–æ—Å—Ç–µ–ª–∏. –Ω–∞ –∫–æ–ª–µ–Ω—è—Ö —Å—Ç–æ—è–ª–∞ –≤–µ—Ä–∞ –∏, —É—Ç–∫–Ω—É–≤—à–∏—Å—å –≤ –º–æ–π –º–∞—Ç—Ä–∞—Ü, –ø–ª–∞–∫–∞–ª–∞. —è ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "if cleaned_texts:\n",
    "    print(\"\\n –ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "    print(\"-\" * 55)\n",
    "    print(cleaned_texts[55][:150] + \"...\" if len(cleaned_texts[0]) > 150 else cleaned_texts[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±—É—á–µ–Ω–∏–µ WordPiece-style —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω\n",
      "--> –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: 3000\n",
      "\n",
      " –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ ./tokenizer:\n",
      "   - tokenizer.json\n",
      "\n",
      " –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: '–≤—ã—Å–æ–∫–æ—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.'\n",
      "   -> –¢–æ–∫–µ–Ω—ã: ['–≤—ã—Å–æ', '##–∫–æ', '##—Ö—É', '##–¥–æ', '##–∂–µ', '##—Å—Ç–≤–µ–Ω', '##–Ω—ã–π', '–ø—Ä–∏', '##–º–µ—Ä', '—Ç–µ', '##–∫', '##—Å—Ç–∞', '–¥–ª—è', '—Ç–æ', '##–∫–µ', '##–Ω–∏–∑', '##–∞', '##—Ü–∏–∏', ',', '–ø—Ä–µ–¥', '##–æ–±', '##—É', '##—á–µ–Ω', '##–∏', '##—è', '–±–æ–ª—å—à–æ–π', '—è–∑—ã', '##–∫–æ–≤–æ', '##–π', '–º–æ', '##–¥–µ–ª–∏', '.']\n",
      "   -> –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: [993, 100, 1150, 137, 184, 910, 223, 143, 440, 728, 52, 186, 385, 126, 313, 1950, 56, 1189, 7, 366, 393, 63, 970, 48, 72, 2205, 1879, 2917, 65, 159, 1891, 9]\n",
      "   -> –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '–≤—ã—Å–æ–∫–æ—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.'\n"
     ]
    }
   ],
   "source": [
    "#  4 : –≤–∞—Ä–∏–∞–Ω—Ç 2\n",
    "# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å WordPiece-style –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece  \n",
    "from tokenizers.trainers import WordPieceTrainer  \n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder  \n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import json\n",
    "\n",
    "class WordPieceTokenizerTrainer:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "    \n",
    "    def train_tokenizer(self, texts: List[str]):\n",
    "        \"\"\"–û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å WordPiece-style –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º\"\"\"\n",
    "        print(\"üîÑ –û–±—É—á–µ–Ω–∏–µ WordPiece-style —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\")\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º WordPiece –º–æ–¥–µ–ª—å\n",
    "        tokenizer = Tokenizer(WordPiece(\n",
    "            unk_token=\"<unk>\",\n",
    "            # –ø—Ä–µ—Ñ–∏–∫—Å \"##\" –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π —Å–ª–æ–≤\n",
    "            continuing_subword_prefix=\"##\"  \n",
    "        ))\n",
    "        \n",
    "        # –ü—Ä–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # WordPiece –¥–µ–∫–æ–¥–µ—Ä - –∑–Ω–∞–µ—Ç –ø—Ä–æ \"##\"\n",
    "        tokenizer.decoder = WordPieceDecoder()\n",
    "        \n",
    "        # –¢—Ä–µ–Ω–µ—Ä –¥–ª—è WordPiece\n",
    "        trainer = WordPieceTrainer(\n",
    "            vocab_size=self.config.VOCAB_SIZE,\n",
    "            min_frequency=2,\n",
    "            special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
    "            continuing_subword_prefix=\"##\"\n",
    "            #max_piece_length=16,  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞\n",
    "        )\n",
    "        \n",
    "        # –û–±—É—á–∞–µ–º –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö\n",
    "        tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "        os.makedirs(self.config.TOKENIZER_SAVE_PATH, exist_ok=True)\n",
    "        tokenizer.save(os.path.join(self.config.TOKENIZER_SAVE_PATH, \"tokenizer.json\"))\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∫ transformers tokenizer\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=os.path.join(self.config.TOKENIZER_SAVE_PATH, \"tokenizer.json\"),\n",
    "            bos_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            mask_token=\"<mask>\",\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω\")\n",
    "        print(f\"--> –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {fast_tokenizer.vocab_size}\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "        print(f\"\\n –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ {self.config.TOKENIZER_SAVE_PATH}:\")\n",
    "        for file in os.listdir(self.config.TOKENIZER_SAVE_PATH):\n",
    "            print(f\"   - {file}\")\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "        test_text = \"–í—ã—Å–æ–∫–æ—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.\"\n",
    "        test_text=test_text.lower()\n",
    "        tokens = fast_tokenizer.tokenize(test_text)\n",
    "        print(f\"\\n –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: '{test_text}'\")\n",
    "        print(f\"   -> –¢–æ–∫–µ–Ω—ã: {tokens}\")\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ\n",
    "        encoded = fast_tokenizer.encode(test_text)\n",
    "        decoded = fast_tokenizer.decode(encoded)\n",
    "        print(f\"   -> –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {encoded}\")\n",
    "        print(f\"   -> –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '{decoded}'\")\n",
    "        \n",
    "        return fast_tokenizer\n",
    "    \n",
    "    def comprehensive_test(self, tokenizer):\n",
    "        \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\"\"\"\n",
    "        test_texts = [\n",
    "            \"–≤—ã—Å–æ–∫–æ—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\",\n",
    "            \"–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏\",\n",
    "            \"–º–∞—à–∏–Ω–∞ –æ–±—É—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ\", \n",
    "            \"—ç—Ç–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤\",\n",
    "            \"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä –∫–∞–∫ –¥–µ–ª–∞\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï WordPiece:\")\n",
    "        \n",
    "        for i, test_text in enumerate(test_texts, 1):\n",
    "            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "            tokens = tokenizer.tokenize(test_text)\n",
    "            \n",
    "            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "            encoded = tokenizer.encode(test_text)\n",
    "            decoded = tokenizer.decode(encoded)\n",
    "            \n",
    "            print(f\"\\n{i}. –û—Ä–∏–≥–∏–Ω–∞–ª: '{test_text}'\")\n",
    "            print(f\"   –¢–æ–∫–µ–Ω—ã: {tokens}\")\n",
    "            print(f\"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '{decoded}'\")\n",
    "            \n",
    "            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            self.analyze_tokens(tokens)\n",
    "            \n",
    "            if decoded == test_text:\n",
    "                print(f\"   ‚úÖ –ü–†–û–ô–î–ï–ù–û\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå –ù–ï –ü–†–û–ô–î–ï–ù–û\")\n",
    "    \n",
    "    def analyze_tokens(self, tokens):\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤\"\"\"\n",
    "        print(f\"   üìä –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
    "        current_word = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\"):\n",
    "                # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–æ–≤–∞\n",
    "                current_word.append(token[2:])  # —É–±–∏—Ä–∞–µ–º \"##\"\n",
    "                print(f\"      '{token}' ‚Üí –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–æ–≤–∞\")\n",
    "            else:\n",
    "                # –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "                if current_word:\n",
    "                    print(f\"      –°–ª–æ–≤–æ: '{''.join(current_word)}'\")\n",
    "                current_word = [token]\n",
    "                print(f\"      '{token}' ‚Üí –Ω–∞—á–∞–ª–æ —Å–ª–æ–≤–∞\")\n",
    "        \n",
    "        if current_word:\n",
    "            print(f\"      –°–ª–æ–≤–æ: '{''.join(current_word)}'\")\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenizer_trainer = WordPieceTokenizerTrainer()\n",
    "tokenizer = tokenizer_trainer.train_tokenizer(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
      "-------------------------------------------------------\n",
      "\n",
      "üîÑ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...\n",
      "\n",
      " -- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 50/107 —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ 13072 —á–∞–Ω–∫–æ–≤\n",
      "\n",
      " -- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 100/107 —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ 25417 —á–∞–Ω–∫–æ–≤\n",
      "\n",
      " -- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 107/107 —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ 27303 —á–∞–Ω–∫–æ–≤\n",
      "\n",
      "‚úÖ –°–æ–∑–¥–∞–Ω–æ 27303 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
      " –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: torch.Size([27303, 512])\n",
      "\n",
      "‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: 27303 –ø—Ä–∏–º–µ—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# 5: \n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, texts: List[str]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = Config()\n",
    "        self.encodings = self._process_texts(texts)\n",
    "    \n",
    "    def _process_texts(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏\"\"\"\n",
    "        all_tokens = []\n",
    "        \n",
    "        print(\"\\nüîÑ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...\")\n",
    "        total_chunks = 0\n",
    "        \n",
    "        for text_idx, text in enumerate(texts):\n",
    "            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "            tokens = self.tokenizer.encode(\n",
    "                text, \n",
    "                add_special_tokens=False,\n",
    "                truncation=False\n",
    "            )\n",
    "            \n",
    "            chunk_size = self.config.CONTEXT_LENGTH - 2  # 512-2 –≥–¥–µ 2-–º–µ—Å—Ç–æ –¥–ª—è BOS –∏ EOS\n",
    "            \n",
    "            for i in range(0, len(tokens), chunk_size):\n",
    "                chunk = tokens[i:i + chunk_size]\n",
    "                if len(chunk) >= 32:  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞\n",
    "                    # –î–æ–±–∞–≤–ª—è–µ–º BOS –∏ EOS\n",
    "                    chunk_with_special = [self.tokenizer.bos_token_id] + chunk + [self.tokenizer.eos_token_id]\n",
    "                    # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "                    if len(chunk_with_special) < self.config.CONTEXT_LENGTH:\n",
    "                        chunk_with_special.extend([self.tokenizer.pad_token_id] * \n",
    "                                                (self.config.CONTEXT_LENGTH - len(chunk_with_special)))\n",
    "                    all_tokens.append(chunk_with_special[:self.config.CONTEXT_LENGTH])\n",
    "                    total_chunks += 1\n",
    "            \n",
    "            if (text_idx + 1) % 50 ==0 or (text_idx + 1) % 107 == 0:\n",
    "                print(f\"\\n -- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {text_idx + 1}/{len(texts)} —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤\")\n",
    "        \n",
    "        input_ids = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"\\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_tokens)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
    "        print(f\" –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {input_ids.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': input_ids.clone()\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'labels': self.encodings['labels'][idx]\n",
    "        }\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "print(\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "print(\"-\" * 55)\n",
    "dataset = TextDataset(tokenizer, cleaned_texts)\n",
    "print(f\"\\n‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "-------------------------------------------------------\n",
      "\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞\n",
      "-- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 132,006,912\n"
     ]
    }
   ],
   "source": [
    "#  6: \n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–ª–±—ç–∫–æ–≤\n",
    "\n",
    "def create_model(tokenizer: PreTrainedTokenizer):\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    \n",
    "    config = LlamaConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=Config.HIDDEN_SIZE,\n",
    "        intermediate_size=Config.INTERMEDIATE_SIZE,\n",
    "        num_hidden_layers=Config.NUM_HIDDEN_LAYERS,\n",
    "        num_attention_heads=Config.NUM_ATTENTION_HEADS,\n",
    "        num_key_value_heads=Config.NUM_KEY_VALUE_HEADS,\n",
    "        max_position_embeddings=Config.CONTEXT_LENGTH,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        tie_word_embeddings=False,\n",
    "    )\n",
    "    \n",
    "    model = LlamaForCausalLM(config)\n",
    "    \n",
    "    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞\")\n",
    "    print(f\"-- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}\")\n",
    "    #print(f\"-- –ü—Ä–∏–º–µ—Ä–Ω–æ {total_params/1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# –ö–æ–ª–ª–±—ç–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "class ValidationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, test_prompts, generation_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.generation_length = generation_length\n",
    "        self.losses = []\n",
    "        self.perplexities = []\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\"\"\"\n",
    "        if logs and 'loss' in logs:\n",
    "            loss = logs['loss']\n",
    "            perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "            self.losses.append(loss)\n",
    "            self.perplexities.append(perplexity)\n",
    "            \n",
    "            if state.global_step % 100 == 0:\n",
    "                print(f\"-> –®–∞–≥ {state.global_step}: Loss = {loss:.4f}, Perplexity = {perplexity:.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–∞—Ö\"\"\"\n",
    "        if state.global_step % 500 == 0:\n",
    "            print(f\"\\n{'='*55}\")\n",
    "            print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ {state.global_step}:\")\n",
    "            print(f\"{'='*55}\")\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, prompt in enumerate(self.test_prompts):\n",
    "                    inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        inputs = inputs.cuda()\n",
    "                        model = model.cuda()\n",
    "                    \n",
    "                    outputs = model.generate(\n",
    "                        inputs,\n",
    "                        max_new_tokens=self.generation_length,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.8,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    )\n",
    "                    \n",
    "                    generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                    print(f\"\\n –ü—Ä–æ–º–ø—Ç {i+1}: {prompt}\")\n",
    "                    print(f\"--> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {generated_text}\")\n",
    "                    print(\"-\" * 55)\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model = create_model(tokenizer)\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–±—ç–∫–∞\n",
    "validation_callback = ValidationCallback(tokenizer, Config.TEST_PROMPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU\n",
      "‚úÖ –¢—Ä–µ–Ω–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n",
      "-- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: 16\n",
      "-- –í—Å–µ–≥–æ —à–∞–≥–æ–≤: 1000\n",
      "-- Learning rate: 0.0005\n",
      "-- Weight decay: 0.1\n"
     ]
    }
   ],
   "source": [
    "#  7 : \n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "\n",
    "def get_training_arguments():\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"--> –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU\")\n",
    "        return TrainingArguments(\n",
    "            output_dir=\"./gpu\",\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=8,\n",
    "            learning_rate=5e-4,\n",
    "            weight_decay=0.1,\n",
    "            max_steps=1000,\n",
    "            warmup_steps=50,\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "            eval_steps=250,\n",
    "            prediction_loss_only=True,\n",
    "            remove_unused_columns=False,\n",
    "            fp16=True,\n",
    "            dataloader_pin_memory=False,\n",
    "            report_to=None,\n",
    "        )\n",
    "    else:\n",
    "        print(\"--> –û–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU\")\n",
    "        return TrainingArguments(\n",
    "            output_dir=\"./cpu\",\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=8,  # —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch 16\n",
    "            learning_rate=5e-4,\n",
    "            weight_decay=0.1,\n",
    "            max_steps=1000,  # –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–∞ CPU\n",
    "            warmup_steps=50,\n",
    "            logging_steps=25,\n",
    "            save_steps=500,\n",
    "            prediction_loss_only=True,\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            report_to=None,\n",
    "        )\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = get_training_arguments()\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer, # –∏–∑–º–µ–Ω–µ–Ω–∏–µ tokenizer=tokenizer\n",
    "    callbacks=[validation_callback],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ –¢—Ä–µ–Ω–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "print(f\"-- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"-- –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {training_args.max_steps}\")\n",
    "print(f\"-- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"-- Weight decay: {training_args.weight_decay}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±—É—á–µ–Ω–∏ –º–æ–¥–µ–ª–∏ ...\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 23:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.904200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> –®–∞–≥ 100: Loss = 6.1468, Perplexity = 467.2199\n",
      "-> –®–∞–≥ 200: Loss = 5.4487, Perplexity = 232.4558\n",
      "-> –®–∞–≥ 300: Loss = 4.9748, Perplexity = 144.7199\n",
      "-> –®–∞–≥ 400: Loss = 4.6723, Perplexity = 106.9434\n",
      "-> –®–∞–≥ 500: Loss = 4.4647, Perplexity = 86.8950\n",
      "-> –®–∞–≥ 600: Loss = 4.2833, Perplexity = 72.4792\n",
      "-> –®–∞–≥ 700: Loss = 4.1443, Perplexity = 63.0735\n",
      "-> –®–∞–≥ 800: Loss = 4.0408, Perplexity = 56.8718\n",
      "-> –®–∞–≥ 900: Loss = 3.9425, Perplexity = 51.5473\n",
      "-> –®–∞–≥ 1000: Loss = 3.9042, Perplexity = 49.6104\n",
      "\n",
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "#  8 : \n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "print(\"üîÑ –û–±—É—á–µ–Ω–∏ –º–æ–¥–µ–ª–∏ ...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./llm_model\n",
      "\n",
      "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏...\n",
      "üîß –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: cuda\n",
      "\n",
      " –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:\n",
      "=======================================================\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 1: –≤—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: –≤—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –∏ –±—ã–ª–∏. –æ–Ω–∏ –∏ –Ω–µ –º–æ–≥–ª–∏ –±—ã –∂–∏—Ç—å –∏, –Ω–æ –Ω–µ –≤ —Å–∏–ª–∞—Ö, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω, –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–µ –º–æ–≥, –∫–∞–∫ —ç—Ç–æ –±—ã–ª–æ –±—ã –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–≥–æ, –Ω–æ –≤ —Å–∞–º–æ–º –¥–µ–ª–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ–Ω —á—É–≤—Å—Ç–≤–æ–≤–∞–ª, –∏ –æ–Ω –±—ã–ª –æ—á–µ–Ω—å –ª–µ–≥–∫–æ. –æ–Ω –Ω–µ –º–æ–≥ —Å–µ–±–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å, —á—Ç–æ –æ–Ω –Ω–µ –∑–Ω–∞–ª, –∫–∞–∫ –æ–Ω —Å–¥–µ–ª–∞–ª. –Ω–æ –∫–æ–≥–¥–∞ –∂–µ –æ–Ω –±—ã–ª –≤ –Ω–µ–º, –æ–Ω —á—É–≤—Å—Ç–≤–æ–≤–∞–ª, —á—Ç–æ –æ–Ω –≤ –ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ, –∫–æ–≥–¥–∞ –æ–Ω, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–Ω –Ω–µ –∑–Ω–∞–ª, –æ–Ω —á—É–≤—Å—Ç–≤–æ–≤–∞–ª\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 2: —Å–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: —Å–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞, –∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—É –∏ –Ω–∞ –≤–∞–º - —Ç–æ –∏ —Ç–µ–ø–µ—Ä—å –∏ –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏–ª–æ, –∞ –æ–Ω –∏ –Ω–µ –¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ –º–æ–≥, –∞ —Ç–æ–ª—å–∫–æ –Ω–∞ —á—Ç–æ—à–Ω–µ –Ω–µ –≤ —á–µ–º - —Ç–æ - —Ç–æ, - –Ω–æ –≤ —Ç–∞–∫–æ–º - —Ç–æ –Ω–µ —Ç–∞–∫–æ–º, –∫–∞–∫ –∏ —á—Ç–æ, –∫–∞–∫ –±—ã –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ, - –∞ —á—Ç–æ, –Ω–∞ —á–µ–º, —Ç–æ, - –≤–æ—Ç —è –∏ –≤ —Ç—É –º–∏–Ω—É—Ç—É –º–∏–Ω—É—Ç—É –Ω–µ –º–æ–≥—É, —á—Ç–æ–±—ã, –æ–¥–Ω–∞–∫–æ, —ç—Ç–æ –Ω–µ –±—É–¥–µ—Ç, - –∞ —è –∏ –Ω–µ –∑–Ω–∞—é\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 3: –º—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: –º—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è –∏, –º–æ–∂–µ—Ç –±—ã—Ç—å, –∏ –Ω–µ –∑–Ω–∞–ª –Ω–∏–∫–∞–∫–æ–≥–æ. - –Ω–µ –∑–Ω–∞—é, —á—Ç–æ —è –≤ —Å–∞–º–æ–º –¥–µ–ª–µ, - —Å–∫–∞–∑–∞–ª–∞ –æ–Ω–∞. - –Ω—É, –∏ —è —Ç–∞–∫ –ª—é–±–∏–ª–∞, –∫–∞–∫ –≤—ã, —á—Ç–æ —É –º–µ–Ω—è, –∫–∞–∫ —è –≤–∞–º –Ω–µ –≥–æ–≤–æ—Ä–∏–ª–∞, —è –¥—É–º–∞—é, —á—Ç–æ —è –≤–∞—Å –ª—é–±–∏—Ç, –∏ –≤—ã –º–µ–Ω—è –Ω–µ –ª—é–±–∏—Ç–µ—Å—å. —è –∏ –Ω–µ –∑–Ω–∞—é, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ, —á—Ç–æ –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ. - —è –Ω–µ –∑–Ω–∞—é, –∫–∞–∫ –≤–∞–º —ç—Ç–æ —Ç–µ–ø–µ—Ä—å —Ç–µ–ø–µ—Ä—å, –∏ —è —Å–∞–º–∞ –Ω–µ –∑–Ω–∞—é. - –Ω–æ —ç—Ç–æ –±—ã–ª–æ –±—ã\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 4: —á–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: —á–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –æ–Ω–∞ –±—ã–ª–∞ –±—ã –ø—Ä–∏–º–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å. –≤—Å–µ —ç—Ç–æ –±—ã–ª–æ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–µ —Ç–æ, —á—Ç–æ –æ–Ω –±—ã–ª —Ç–∞–∫, –∏ –≤–¥—Ä—É–≥ –Ω–µ –º–æ–≥ –≤—ã–π—Ç–∏, —á—Ç–æ –æ–Ω –∏ —Ç–µ–ø–µ—Ä—å, –∏ –≤—Å—ë—Ç–∞–∫–∏ –≤ —ç—Ç–æ–º —Ä–æ–¥–µ, –∏ –≤—Å—ë, —á—Ç–æ –æ–Ω –≤—Å–µ –∑–Ω–∞–ª, —á—Ç–æ –≤ –Ω–µ–π –±—ã–ª–æ —Ç–æ, —á—Ç–æ –æ–Ω –Ω–µ –º–æ–≥ –ø–æ–Ω—è—Ç—å –∏ —á—Ç–æ –æ–Ω –≥–æ–≤–æ—Ä–∏–ª, –∞ —Ç–µ–ø–µ—Ä—å, –Ω–µ —Ç–æ, —á—Ç–æ –æ–Ω –Ω–∏–∫–æ–≥–¥–∞ –∏ –Ω–µ –∑–Ω–∞–ª, —á—Ç–æ –æ–Ω —Ç–∞–∫ –∏ –Ω–µ –º–æ–≥, —á—Ç–æ –µ–º—É –±—ã–ª–æ —É–∂–µ –Ω–µ –Ω—É–∂–Ω–æ, –Ω–æ –æ–Ω –Ω–µ\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 5: —á—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: —á—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É —Ç–µ–ø–µ—Ä—å, –Ω–æ –º–Ω–µ —Ö–æ—Ç–µ–ª–æ—Å—å, —á—Ç–æ —è –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å, –∏ —á—Ç–æ –æ–Ω, —Ç–∞–∫ –∫–∞–∫ –±—ã –≤ —á–µ–º–æ—Ä–µ, –∫–∞–∫ —è –≤ —Ç–∞–∫–æ–º –¥–µ–ª–µ, —Ç–∞–∫ —á—Ç–æ, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–µ –±—ã–ª–æ. –∫–∞–∫ —ç—Ç–æ —Ç–∞–∫, —á—Ç–æ –≤ –Ω–µ–π –µ—Å—Ç—å, –∏, –∫–æ–Ω–µ—á–Ω–æ, –∏ –Ω–µ –Ω—É–∂–Ω–æ –±—ã–ª–æ, –∫–∞–∫ –∏ —á—Ç–æ –æ–Ω, –µ—Å–ª–∏ –±—ã –æ–Ω –Ω–µ –º–æ–≥ –ø–æ–Ω—è—Ç—å, –æ–Ω, –µ—Å–ª–∏, –Ω–µ –º–æ–∂–µ—Ç –ª–∏ –±—ã–ª–æ, –æ–Ω –Ω–µ –∑–Ω–∞–µ—Ç, —á—Ç–æ –æ–Ω —É–∂–µ –Ω–µ –º–æ–∂–µ—Ç, –∏ —á—Ç–æ –æ–Ω, –º–æ–∂–µ—Ç –±—ã—Ç—å, –∏ –Ω–µ –º–æ–≥ –±—ã –¥—É–º–∞—Ç—å\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 6: –ª—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: –ª—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏, –Ω–æ —Å —Ä–∞–∑—Ä—É–∫–∏–≤–∞–ª–∏ –∏ –∑–∞–ø–µ—Ä—Ç–∏—Ç. - –¥–∞ –∏ –Ω–µ –≤ –≤–∞—à–µ–º –¥–µ–ª–æ. - –∞ —Ç–æ –∏ –Ω–µ —Å —Ç–æ–±–æ–π. - –Ω—É, –∫–∞–∫ –∂–µ. - –¥–∞. –∏ –Ω–µ –Ω–∞–¥–æ, - –∞ —á—Ç–æ –∂–µ –≤—ã, –∫–∞–∫ –±—ã –Ω–∏ –≤ —á–µ–º –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ. - –¥–∞, —è, –ø–æ–∂–∞–ª—É–π, –Ω–µ –±–æ—é—Å—å, - –≤–æ–∑—Ä–∞–∑–∏–ª –æ–Ω. - —á—Ç–æ –∂–µ –≤—ã –Ω–µ —Ö–æ—Ç–µ–ª–∏ –ø—Ä–æ–π—Ç–∏. - –æ–Ω –∏ –Ω–µ –∑–Ω–∞–µ—Ç. - —á—Ç–æ –∂ —ç—Ç–æ –≤—ã. - —è –±—ã –Ω–µ\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 7: –Ω–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: –Ω–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞, - –∏ —Ç–µ–ø–µ—Ä—å, –∫–∞–∫ –≤—Å–µ–≥–¥–∞, –∫–∞–∫ –±—É–¥—Ç–æ –±—ã –±—ã–ª–æ. - –¥–∞, –¥–∞, - —Å–∫–∞–∑–∞–ª –∫–Ω—è–∑—å –∞–Ω–¥—Ä–µ–π. - –¥–∞, —è –æ—á–µ–Ω—å —Ä–∞–¥, —á—Ç–æ, –∏ –≤—ã –Ω–µ —Ö–æ—Ç–∏—Ç–µ, –∞ –≤–∞–º, –∫–∞–∂–µ—Ç—Å—è, –∏ –Ω–µ –ø–æ–Ω–∏–º–∞—é. - –∏ —á—Ç–æ –∂–µ, –µ—Å–ª–∏ –≤–∞–º –Ω–µ —Ö–æ—Ç–µ–ª–æ—Å—å. - –≤—ã –∑–Ω–∞–µ—Ç–µ, —á—Ç–æ –≤—ã –∑–Ω–∞–µ—Ç–µ. - —è –æ—á–µ–Ω—å –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ –≤—ã –≤—Å–µ - —Ç–∞–∫–∏ –º–æ–∂–µ—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å. - –Ω–µ—Ç, —è –¥—É–º–∞—é, —è –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–Ω–∏–º–∞—é. - —è –Ω–µ –∑–Ω–∞—é, - —Å–∫–∞–∑–∞–ª–∞ —è. -\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 8: –≤—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: –≤—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—Å—é –∂–∏–∑–Ω—å, –∫–∞–∫ –±—ã –Ω–µ —Å–º–µ—è –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –Ω–∏ –Ω–∏ –Ω–∞ —á–µ–º–æ—Ä–µ, –Ω–∏ —Å –∫–æ—Ç–æ—Ä—ã–º–∏–º –∏ –Ω–µ —Å—á–∏—Ç–∞—è –≤ —ç—Ç–æ–º —Ä–æ–¥–µ ; –∏, –∫—Ä–æ–º–µ —Ç–æ–≥–æ, –∏ –≤—Å–µ, —á—Ç–æ –æ–Ω –≤–∏–¥–µ–ª, –∏ —á—Ç–æ —Ç–µ–ø–µ—Ä—å, –∏ –≤—Å–µ —ç—Ç–∏ –ª—é–¥–∏, –∏ –Ω–µ –∏–º–µ–ª –Ω–∏ –≤ —á–µ–º–æ—Ä–µ, –Ω–∏ –≤ —á–µ–º–æ—Ä–µ, –Ω–∏ –≤ —á–µ–º–æ—Ä–∞—Ö. –∞ –≤ —Å–∞–º–æ–º –¥–µ–ª–µ, –∫–∞–∫ –±—É–¥—Ç–æ –±—ã –Ω–µ –±—ã–ª–æ, –æ–Ω, –∏ –∫–∞–∫ –±—ã –∏ –Ω–µ –º–æ–≥ –±—ã —Å–¥–µ–ª–∞—Ç—å\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 9: –≤–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: –≤–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ, - –≥–æ–≤–æ—Ä–∏–ª –∏ –µ—Å—Ç—å, —á—Ç–æ –∂–µ –≤ –Ω–µ–º—Ä–∞–µ—Ç, —á—Ç–æ –≤ —Å–∞–º–æ–º –¥–µ–ª–µ –≤—Ä–µ–º—è, –∞ –Ω–µ –≤—Å–µ. —ç—Ç–æ —è –≤–∞–º, - —Å–∫–∞–∑–∞–ª –æ–Ω, - –Ω–æ —è –Ω–µ –∑–Ω–∞—é, - –∫–∞–∫ –∂–µ —ç—Ç–æ –≤ —Ç–æ–º, —á—Ç–æ —è –±—É–¥—É –≥–æ–≤–æ—Ä–∏—Ç—å —Å –Ω–∏–º. –∏, –µ—Å–ª–∏ —è –∏ –Ω–µ –¥—É–º–∞—é, —á—Ç–æ –æ–Ω —Ç–∞–∫, –∫–∞–∫ –±—É–¥—Ç–æ –∏ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç, –∫–∞–∫ –±—ã –Ω–∏ –≤ —á–µ–º –Ω–µ —Ç–æ, —á—Ç–æ, –∫—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ –ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ, –∏ —á—Ç–æ –æ–Ω, –∏ —á—Ç–æ —ç—Ç–æ –æ–Ω\n",
      "-------------------------------------------------------\n",
      "\n",
      " –ü—Ä–æ–º–ø—Ç 10: —á—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\n",
      "-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: —á—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ, —è –∏ –∑–∞ –≤–∞–º–∏, –∏ –≤—Å–µ –Ω–∞ –º–µ–Ω—è –∏ –∑–∞ —Ç–æ, —á—Ç–æ —è –Ω–µ –∑–Ω–∞—é, –Ω–µ —Ö–æ—á—É, —á—Ç–æ —è —Ç–æ–ª—å–∫–æ, —á—Ç–æ –æ–Ω–∞ –Ω–µ –º–æ–≥–ª–∞ –±—ã –±—ã—Ç—å, —á—Ç–æ –æ–Ω –Ω–µ –º–æ–≥ –Ω–µ –º–æ–≥ —Å–µ–±–µ. –µ—Å–ª–∏ —è –Ω–µ –∑–Ω–∞—é, –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–∞–∂–µ –∏ –Ω–µ –≤–∏–¥–∞–ª –Ω–∏—á–µ–≥–æ, –Ω–µ –∑–Ω–∞—é. —è –Ω–µ –∑–Ω–∞—é, —á—Ç–æ —è –Ω–µ —Ö–æ—á—É –∂–∏—Ç—å —Å –Ω–µ—é, –∏ —è —Å–∞–º –Ω–µ –∑–Ω–∞–ª–∞, —á—Ç–æ –æ–Ω, –∫—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–∂–µ—Ç –±—ã—Ç—å, –∏ –Ω–µ –∑–Ω–∞–ª, —á—Ç–æ —è –±—ã –∏ –Ω–∏—Å–∫–æ–ª—å–∫–æ –Ω–µ –∂–µ–ª–∞–ª\n",
      "-------------------------------------------------------\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./llm_model/final_results.json\n"
     ]
    }
   ],
   "source": [
    "#  9 : \n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞\n",
    "\n",
    "print(\"üîÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "trainer.save_model(Config.MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(Config.MODEL_SAVE_PATH)\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {Config.MODEL_SAVE_PATH}\")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞\n",
    "class QualityEvaluator:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(f\"üîß –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: {self.device}\")\n",
    "    \n",
    "    def evaluate_generation(self, max_length=100):\n",
    "        \"\"\"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\"\"\"\n",
    "        print(\"\\n –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:\")\n",
    "        print(\"=\" * 55)\n",
    "        \n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, prompt in enumerate(Config.TEST_PROMPTS):\n",
    "                prompt=prompt.lower()\n",
    "                inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                \n",
    "                inputs = inputs.to(self.device)\n",
    "                \n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.85,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "                \n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                print(f\"\\n –ü—Ä–æ–º–ø—Ç {i+1}: {prompt}\")\n",
    "                print(f\"-> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é —Ç–µ–∫—Å—Ç: {generated_text}\")\n",
    "                print(\"-\" * 55)\n",
    "                \n",
    "                results.append({\n",
    "                    'prompt': prompt,\n",
    "                    'generated_text_raw': generated_text\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "print(\"\\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏...\")\n",
    "trained_model = LlamaForCausalLM.from_pretrained(Config.MODEL_SAVE_PATH)\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_SAVE_PATH)\n",
    "\n",
    "evaluator = QualityEvaluator(trained_tokenizer, trained_model)\n",
    "final_results = evaluator.evaluate_generation()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "with open(f\"{Config.MODEL_SAVE_PATH}/final_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {Config.MODEL_SAVE_PATH}/final_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
